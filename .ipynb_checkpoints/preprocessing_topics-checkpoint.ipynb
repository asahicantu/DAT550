{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessry libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\python\\382\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python\\382\\lib\\site-packages (from wordcloud) (3.1.3)\n",
      "Requirement already satisfied: pillow in c:\\python\\382\\lib\\site-packages (from wordcloud) (7.1.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\python\\382\\lib\\site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\python\\382\\lib\\site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\382\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\python\\382\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\382\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\python\\382\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\382\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in c:\\python\\382\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python\\382\\lib\\site-packages (from seaborn) (1.18.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\python\\382\\lib\\site-packages (from seaborn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\python\\382\\lib\\site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in c:\\python\\382\\lib\\site-packages (from seaborn) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\python\\382\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\python\\382\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\python\\382\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\382\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\382\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\382\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\382\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (41.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\python\\382\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\python\\382\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wordcloud\n",
    "%pip install seaborn\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles.csv', error_bad_lines=False, nrows=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>word count   text word count     authorfunder ...</td>\n",
       "      <td>vp and vp which is further processed to vp and...</td>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>241</td>\n",
       "      <td>1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>during the past three months a new coronavirus...</td>\n",
       "      <td>in december  a novel coronavirus sarscov was i...</td>\n",
       "      <td>00340eea543336d54adda18236424de6a5e91c9d</td>\n",
       "      <td>175</td>\n",
       "      <td>2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the ncov epidemic has spread across china and ...</td>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the fast accumulation of viral metagenomic dat...</td>\n",
       "      <td>metagenomic sequencing which allows us to dire...</td>\n",
       "      <td>00911cf4f99a3d5ae5e5b787675646a743574496</td>\n",
       "      <td>139</td>\n",
       "      <td>5188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>infectious bronchitis ib causes significant ec...</td>\n",
       "      <td>infectious bronchitis ib which is caused by in...</td>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>1647</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0           0  word count   text word count     authorfunder ...   \n",
       "1           1  during the past three months a new coronavirus...   \n",
       "2           2                                                NaN   \n",
       "3           3  the fast accumulation of viral metagenomic dat...   \n",
       "4           4  infectious bronchitis ib causes significant ec...   \n",
       "\n",
       "                                                body  \\\n",
       "0  vp and vp which is further processed to vp and...   \n",
       "1  in december  a novel coronavirus sarscov was i...   \n",
       "2  the ncov epidemic has spread across china and ...   \n",
       "3  metagenomic sequencing which allows us to dire...   \n",
       "4  infectious bronchitis ib which is caused by in...   \n",
       "\n",
       "                                   paper_id  abstract_word_count  \\\n",
       "0  0015023cc06b5362d332b3baf348d11567ca2fbb                  241   \n",
       "1  00340eea543336d54adda18236424de6a5e91c9d                  175   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76                    0   \n",
       "3  00911cf4f99a3d5ae5e5b787675646a743574496                  139   \n",
       "4  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b                 1647   \n",
       "\n",
       "   body_word_count  \n",
       "0             1728  \n",
       "1             2549  \n",
       "2              755  \n",
       "3             5188  \n",
       "4             4003  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new dataframe with the column i need and delete NAN values of abstracts\n",
    "abstracts = data[['abstract']]\n",
    "abstracts = abstracts.dropna()\n",
    "documents = abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "1. Tokenization/lemmatization/lowercase\n",
    "2. Stopwords removed \n",
    "3. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(word,pos): \n",
    "    '''\n",
    "        the lemmatizer takes the 'word' and based on the 'pos' finds the base word of the 'word'\n",
    "        the pos defines weather to look for a verb, noun, etc. \n",
    "    '''\n",
    "    lem = WordNetLemmatizer()\n",
    "    return lem.lemmatize(word, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordStemmer(word): \n",
    "    '''\n",
    "        the wordstemmer, returns the stem of the word 'word'\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    wordstem = stemmer.stem(word)\n",
    "    return wordstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textpreprocessing(text):\n",
    "    '''\n",
    "        takes in a 'text',tokenize it, remove stopwords, lemmatize, and finds the stem of each word. \n",
    "        Returns the preprocessed text\n",
    "    '''\n",
    "    result = []\n",
    "    for token in word_tokenize(text):\n",
    "        if token not in set(stopwords.words('english')):\n",
    "            txt = lemmatizer(token, 'v')\n",
    "            txt = wordStemmer(txt)\n",
    "            result.append(txt)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     word count   text word count     authorfunder ...\n",
       "1     during the past three months a new coronavirus...\n",
       "3     the fast accumulation of viral metagenomic dat...\n",
       "4     infectious bronchitis ib causes significant ec...\n",
       "5     nipah virus niv came into limelight recently d...\n",
       "6     background a novel coronavirus ncov emerged in...\n",
       "7     all rights reserved no reuse allowed without p...\n",
       "8     faced with the current largescale public healt...\n",
       "9     viruses interact with hundreds to thousands of...\n",
       "10    school closure is often considered as an optio...\n",
       "11    posttranscriptional gene silencing ptgs is a p...\n",
       "13    objective to review and critically appraise pu...\n",
       "14    human astroviruses are small nonenveloped viru...\n",
       "15    ribosomal frameshifting during the translation...\n",
       "16    the recent outbreak of infections by the  nove...\n",
       "17    in the note the logistic growth regression mod...\n",
       "19    the paper evaluates total inclusive costs of t...\n",
       "21    an outbreak of a novel coronavirus named covid...\n",
       "22    longread sequencing technologies are invaluabl...\n",
       "23    ncov has caused more than  deaths as of  janua...\n",
       "25    the basic reproductive number r  is one of the...\n",
       "26    to ultimately combat the emerging covid pandem...\n",
       "27    the newly emergent human virus sarscov is resu...\n",
       "28    many pathogens take advantage of the dependenc...\n",
       "29    the ongoing zika epidemic in the americas has ...\n",
       "30    covid threatens to overwhelm hospital faciliti...\n",
       "31    in this study we reported for the first time t...\n",
       "32    background the novel coronavirus sarscov pande...\n",
       "33    in the wake of the rohingya populations mass m...\n",
       "34    based on publicly released data for  patients ...\n",
       "35    a new porcine coronavirus sadscov was recently...\n",
       "36     a subset of intracellular monoadpribosyltrans...\n",
       "38    we analyzed agespecific and sexspecific morbid...\n",
       "39    strict interventions were successful to contro...\n",
       "41     words  quantification of aerosolized influenz...\n",
       "42    with the emergence of  rd generation transmiss...\n",
       "43    motivation understanding the relationship betw...\n",
       "44    the world health organization who has declared...\n",
       "45    two months after it was firstly reported the n...\n",
       "46    with the ongoing outbreak of coronavirus disea...\n",
       "47    doi biorxiv preprint  components allan and met...\n",
       "48    since covid emerged in early december  in wuha...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\admin/nltk_data'\n    - 'c:\\\\python\\\\382\\\\nltk_data'\n    - 'c:\\\\python\\\\382\\\\share\\\\nltk_data'\n    - 'c:\\\\python\\\\382\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\admin/nltk_data'\n    - 'c:\\\\python\\\\382\\\\nltk_data'\n    - 'c:\\\\python\\\\382\\\\share\\\\nltk_data'\n    - 'c:\\\\python\\\\382\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-69dbef062497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# preprocess all the abstracts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprocessed_abstracts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextpreprocessing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3628\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3629\u001b[0m         \"\"\"\n\u001b[1;32m-> 3630\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3631\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-54fc9f377f21>\u001b[0m in \u001b[0;36mtextpreprocessing\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'v'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\382\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\admin/nltk_data'\n    - 'c:\\\\python\\\\382\\\\nltk_data'\n    - 'c:\\\\python\\\\382\\\\share\\\\nltk_data'\n    - 'c:\\\\python\\\\382\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\admin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# preprocess all the abstracts\n",
    "processed_abstracts = documents['abstract'].map(textpreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_abstracts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c37fffd52055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocessed_abstracts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_abstracts' is not defined"
     ]
    }
   ],
   "source": [
    "processed_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this dictionary we save all the tokens in the processed_abstracts\n",
    "# The dictionary saves an ID for each token/word from the docs\n",
    "dictionary = gensim.corpora.Dictionary(processed_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_bow = [dictionary.doc2bow(doc) for doc in processed_abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \n",
    "coronavirus related words\n",
    "-virus\n",
    "-viral \n",
    "-coronavirus\n",
    "-respiratori\n",
    "-pneumonia\n",
    "-lungs\n",
    "-outbreak\n",
    "-pandem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize token frequenxy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together all tokens from all abstracts in a single list\n",
    "abstract_tokens = []\n",
    "for i in processed_abstracts:\n",
    "    for j in i:\n",
    "        abstract_tokens.append(j)\n",
    "\n",
    "\n",
    "# Using countvectorizer to vectorize the abstract tokens, as it counts the term frequency \n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer_data = vectorizer.fit_transform(abstract_tokens)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "counts = np.zeros(len(feature_names))\n",
    "for i in vectorizer_data:\n",
    "    counts+= i.toarray()[0]\n",
    "    \n",
    "\n",
    "sorted_dict = sorted((zip(feature_names, counts)), key=lambda x:x[1], reverse=True)[0:25]\n",
    "words = [w[0] for w in sorted_dict]\n",
    "word_counts = [w[1] for w in sorted_dict]\n",
    "x_pos = np.arange(len(words)) \n",
    "\n",
    "# The plot shows the most frequent words used in all the abstracts\n",
    "#plt.figure(2, figsize=(15, 7))\n",
    "#sns.barplot(x_pos, word_counts, palette='hls')\n",
    "#plt.xticks(x_pos, words, rotation=90) \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the whole collection to a single string to use in the wordcloud\n",
    "# can make this later for each cluster if we want\n",
    "docs = pd.DataFrame(processed_abstracts)\n",
    "allabstracts = \"\"\n",
    "for i in processed_abstracts:\n",
    "    for j in i:\n",
    "        allabstracts += j\n",
    "        allabstracts += ' '\n",
    "\n",
    "    \n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=200)\n",
    "wordcloud.generate(allabstracts)\n",
    "wordcloud.to_image()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.figure(figsize=[20,20])\n",
    "#plt.imshow(wordcloud, interpolation=\"sinc\")\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "# First fit the dict abstract_bow using the tfidfmodel \n",
    "tfidf_model = TfidfModel(abstract_bow)\n",
    "\n",
    "# now apply the tfidf_model to the documents\n",
    "corpus_tfidf = tfidf_model[abstract_bow]   # can here just try to apply to the first doc in corpus to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(abstract_bow, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0      Word:  0.004*\"patient\" + 0.004*\"qingdao\" + 0.004*\"perpetu\" + 0.004*\"grant\" + 0.004*\"right\" + 0.004*\"reus\" + 0.004*\"permiss\" + 0.004*\"reserv\" + 0.003*\"medrxiv\" + 0.003*\"display\"\n",
      "\n",
      "Topic: 1      Word:  0.004*\"r\" + 0.004*\"protein\" + 0.004*\"nsp\" + 0.003*\"vaccin\" + 0.003*\"secondari\" + 0.002*\"predict\" + 0.002*\"number\" + 0.002*\"heterogen\" + 0.002*\"moment\" + 0.002*\"emerg\"\n",
      "\n",
      "Topic: 2      Word:  0.005*\"contact\" + 0.003*\"social\" + 0.003*\"distanc\" + 0.003*\"contacttrac\" + 0.002*\"transmiss\" + 0.002*\"intervent\" + 0.002*\"control\" + 0.002*\"epidem\" + 0.002*\"covid\" + 0.002*\"suscept\"\n",
      "\n",
      "Topic: 3      Word:  0.004*\"astrovirus\" + 0.004*\"filter\" + 0.004*\"mutat\" + 0.003*\"nvis\" + 0.003*\"bind\" + 0.003*\"recoveri\" + 0.002*\"compound\" + 0.002*\"niv\" + 0.002*\"druglik\" + 0.002*\"priorit\"\n",
      "\n",
      "Topic: 4      Word:  0.001*\"wuhan\" + 0.001*\"ncov\" + 0.001*\"protein\" + 0.001*\"outbreak\" + 0.001*\"preprint\" + 0.001*\"new\" + 0.001*\"covid\" + 0.001*\"high\" + 0.001*\"perpetu\" + 0.001*\"identifi\"\n",
      "\n",
      "Topic: 5      Word:  0.006*\"frameshift\" + 0.004*\"patient\" + 0.003*\"travel\" + 0.003*\"henan\" + 0.002*\"covid\" + 0.002*\"restrict\" + 0.002*\"translat\" + 0.002*\"incub\" + 0.002*\"rna\" + 0.002*\"mobil\"\n",
      "\n",
      "Topic: 6      Word:  0.004*\"correct\" + 0.003*\"refuge\" + 0.003*\"mngs\" + 0.003*\"popul\" + 0.003*\"vaccin\" + 0.003*\"bed\" + 0.003*\"technolog\" + 0.003*\"ncov\" + 0.002*\"measl\" + 0.002*\"design\"\n",
      "\n",
      "Topic: 7      Word:  0.004*\"sulfat\" + 0.003*\"heparin\" + 0.003*\"heparan\" + 0.003*\"polysaccharid\" + 0.002*\"surfac\" + 0.002*\"receptor\" + 0.002*\"interact\" + 0.001*\"bind\" + 0.001*\"domain\" + 0.001*\"hsr\"\n",
      "\n",
      "Topic: 8      Word:  0.004*\"pro\" + 0.004*\"]\" + 0.004*\"[\" + 0.003*\"admit\" + 0.003*\"patient\" + 0.003*\"januari\" + 0.003*\"drug\" + 0.002*\"nelfinavir\" + 0.002*\"kcalmol\" + 0.002*\"dock\"\n",
      "\n",
      "Topic: 9      Word:  0.004*\"ibv\" + 0.004*\"protein\" + 0.003*\"isol\" + 0.003*\"adapt\" + 0.003*\"virusinteract\" + 0.003*\"strategi\" + 0.002*\"mammal\" + 0.002*\"death\" + 0.002*\"focus\" + 0.002*\"sampl\"\n",
      "\n",
      "Topic: 10      Word:  0.004*\"end\" + 0.004*\"citi\" + 0.004*\"background\" + 0.003*\"humantohuman\" + 0.003*\"ncov\" + 0.003*\"wuhan\" + 0.002*\"emerg\" + 0.002*\"caus\" + 0.002*\"china\" + 0.002*\"novel\"\n",
      "\n",
      "Topic: 11      Word:  0.004*\"cheer\" + 0.003*\"logist\" + 0.003*\"cpsrnas\" + 0.003*\"taxonom\" + 0.003*\"rnas\" + 0.003*\"psrnas\" + 0.003*\"sarscovcpsr\" + 0.003*\"palindrom\" + 0.002*\"size\" + 0.002*\"south\"\n",
      "\n",
      "Topic: 12      Word:  0.002*\"sadscov\" + 0.002*\"coronavirus\" + 0.002*\"twirl\" + 0.002*\"januari\" + 0.002*\"wuhan\" + 0.002*\"hku\" + 0.002*\"porcin\" + 0.002*\"inform\" + 0.002*\"bat\" + 0.002*\"confirm\"\n",
      "\n",
      "Topic: 13      Word:  0.004*\"cost\" + 0.004*\"sirna\" + 0.003*\"cl\" + 0.003*\"deliveri\" + 0.003*\"plant\" + 0.003*\"proteas\" + 0.003*\"peak\" + 0.002*\"total\" + 0.002*\"approach\" + 0.002*\"measur\"\n",
      "\n",
      "Topic: 14      Word:  0.004*\"morbid\" + 0.004*\"zika\" + 0.004*\"artd\" + 0.003*\"lifespan\" + 0.003*\"america\" + 0.003*\"cohort\" + 0.003*\"rok\" + 0.003*\"year\" + 0.002*\"meet\" + 0.002*\"rap\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows 10 topics extracted from the abstracts\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {}      Word:  {}'.format(idx, topic) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
