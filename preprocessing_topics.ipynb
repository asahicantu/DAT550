{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessry libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles.csv', error_bad_lines=False, nrows=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>word count   text word count     authorfunder ...</td>\n",
       "      <td>vp and vp which is further processed to vp and...</td>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>241</td>\n",
       "      <td>1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>during the past three months a new coronavirus...</td>\n",
       "      <td>in december  a novel coronavirus sarscov was i...</td>\n",
       "      <td>00340eea543336d54adda18236424de6a5e91c9d</td>\n",
       "      <td>175</td>\n",
       "      <td>2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the ncov epidemic has spread across china and ...</td>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>the fast accumulation of viral metagenomic dat...</td>\n",
       "      <td>metagenomic sequencing which allows us to dire...</td>\n",
       "      <td>00911cf4f99a3d5ae5e5b787675646a743574496</td>\n",
       "      <td>139</td>\n",
       "      <td>5188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>infectious bronchitis ib causes significant ec...</td>\n",
       "      <td>infectious bronchitis ib which is caused by in...</td>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>1647</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           abstract  \\\n",
       "0           0  word count   text word count     authorfunder ...   \n",
       "1           1  during the past three months a new coronavirus...   \n",
       "2           2                                                NaN   \n",
       "3           3  the fast accumulation of viral metagenomic dat...   \n",
       "4           4  infectious bronchitis ib causes significant ec...   \n",
       "\n",
       "                                                body  \\\n",
       "0  vp and vp which is further processed to vp and...   \n",
       "1  in december  a novel coronavirus sarscov was i...   \n",
       "2  the ncov epidemic has spread across china and ...   \n",
       "3  metagenomic sequencing which allows us to dire...   \n",
       "4  infectious bronchitis ib which is caused by in...   \n",
       "\n",
       "                                   paper_id  abstract_word_count  \\\n",
       "0  0015023cc06b5362d332b3baf348d11567ca2fbb                  241   \n",
       "1  00340eea543336d54adda18236424de6a5e91c9d                  175   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76                    0   \n",
       "3  00911cf4f99a3d5ae5e5b787675646a743574496                  139   \n",
       "4  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b                 1647   \n",
       "\n",
       "   body_word_count  \n",
       "0             1728  \n",
       "1             2549  \n",
       "2              755  \n",
       "3             5188  \n",
       "4             4003  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new dataframe with the column i need and delete NAN values of abstracts\n",
    "abstracts = data[['abstract']]\n",
    "abstracts = abstracts.dropna()\n",
    "documents = abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "1. Tokenization/lemmatization/lowercase\n",
    "2. Stopwords removed \n",
    "3. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(word,pos): \n",
    "    '''\n",
    "        the lemmatizer takes the 'word' and based on the 'pos' finds the base word of the 'word'\n",
    "        the pos defines weather to look for a verb, noun, etc. \n",
    "    '''\n",
    "    lem = WordNetLemmatizer()\n",
    "    return lem.lemmatize(word, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordStemmer(word): \n",
    "    '''\n",
    "        the wordstemmer, returns the stem of the word 'word'\n",
    "    '''\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    wordstem = stemmer.stem(word)\n",
    "    return wordstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textpreprocessing(text):\n",
    "    '''\n",
    "        takes in a 'text',tokenize it, remove stopwords, lemmatize, and finds the stem of each word. \n",
    "        Returns the preprocessed text\n",
    "    '''\n",
    "    result = []\n",
    "    for token in word_tokenize(text):\n",
    "        if token not in set(stopwords.words('english')):\n",
    "            txt = lemmatizer(token, 'v')\n",
    "            txt = wordStemmer(txt)\n",
    "            result.append(txt)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess all the abstracts\n",
    "processed_abstracts = documents['abstract'].map(textpreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [word, count, text, word, count, authorfund, r...\n",
       "1     [past, three, month, new, coronavirus, sarscov...\n",
       "3     [fast, accumul, viral, metagenom, data, contri...\n",
       "4     [infecti, bronchiti, ib, caus, signific, econo...\n",
       "5     [nipah, virus, niv, come, limelight, recent, d...\n",
       "6     [background, novel, coronavirus, ncov, emerg, ...\n",
       "7     [right, reserv, reus, allow, without, permiss,...\n",
       "8     [face, current, largescal, public, health, eme...\n",
       "9     [virus, interact, hundr, thousand, protein, ma...\n",
       "10    [school, closur, often, consid, option, mitig,...\n",
       "11    [posttranscript, gene, silenc, ptgs, power, to...\n",
       "13    [object, review, critic, apprais, publish, pre...\n",
       "14    [human, astrovirus, small, nonenvelop, virus, ...\n",
       "15    [ribosom, frameshift, translat, rna, implic, h...\n",
       "16    [recent, outbreak, infect, novel, coronavirus,...\n",
       "17    [note, logist, growth, regress, model, use, es...\n",
       "19    [paper, evalu, total, inclus, cost, three, pub...\n",
       "21    [outbreak, novel, coronavirus, name, covid, fi...\n",
       "22    [longread, sequenc, technolog, invalu, determi...\n",
       "23    [ncov, caus, death, januari, china, infect, ca...\n",
       "25    [basic, reproduct, number, r, one, common, com...\n",
       "26    [ultim, combat, emerg, covid, pandem, desir, d...\n",
       "27    [newli, emerg, human, virus, sarscov, result, ...\n",
       "28    [mani, pathogen, take, advantag, depend, host,...\n",
       "29    [ongo, zika, epidem, america, challeng, public...\n",
       "30    [covid, threaten, overwhelm, hospit, facil, th...\n",
       "31    [studi, report, first, time, exist, complement...\n",
       "32    [background, novel, coronavirus, sarscov, pand...\n",
       "33    [wake, rohingya, popul, mass, migrat, myanmar,...\n",
       "34    [base, public, releas, data, patient, investig...\n",
       "35    [new, porcin, coronavirus, sadscov, recent, id...\n",
       "36    [subset, intracellular, monoadpribosyltransfer...\n",
       "38    [analyz, agespecif, sexspecif, morbid, mortal,...\n",
       "39    [strict, intervent, success, control, novel, c...\n",
       "41    [word, quantif, aerosol, influenza, virus, use...\n",
       "42    [emerg, rd, generat, transmiss, prevent, treat...\n",
       "43    [motiv, understand, relationship, sequenc, str...\n",
       "44    [world, health, organ, declar, novel, coronavi...\n",
       "45    [two, month, first, report, novel, coronavirus...\n",
       "46    [ongo, outbreak, coronavirus, diseas, infect, ...\n",
       "47    [doi, biorxiv, preprint, compon, allan, meet, ...\n",
       "48    [sinc, covid, emerg, earli, decemb, wuhan, swe...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this dictionary we save all the tokens in the processed_abstracts\n",
    "# The dictionary saves an ID for each token/word from the docs\n",
    "dictionary = gensim.corpora.Dictionary(processed_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_bow = [dictionary.doc2bow(doc) for doc in processed_abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \n",
    "coronavirus related words\n",
    "-virus\n",
    "-viral \n",
    "-coronavirus\n",
    "-respiratori\n",
    "-pneumonia\n",
    "-lungs\n",
    "-outbreak\n",
    "-pandem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize token frequenxy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together all tokens from all abstracts in a single list\n",
    "abstract_tokens = []\n",
    "for i in processed_abstracts:\n",
    "    for j in i:\n",
    "        abstract_tokens.append(j)\n",
    "\n",
    "\n",
    "# Using countvectorizer to vectorize the abstract tokens, as it counts the term frequency \n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer_data = vectorizer.fit_transform(abstract_tokens)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "counts = np.zeros(len(feature_names))\n",
    "for i in vectorizer_data:\n",
    "    counts+= i.toarray()[0]\n",
    "    \n",
    "\n",
    "sorted_dict = sorted((zip(feature_names, counts)), key=lambda x:x[1], reverse=True)[0:25]\n",
    "words = [w[0] for w in sorted_dict]\n",
    "word_counts = [w[1] for w in sorted_dict]\n",
    "x_pos = np.arange(len(words)) \n",
    "\n",
    "# The plot shows the most frequent words used in all the abstracts\n",
    "#plt.figure(2, figsize=(15, 7))\n",
    "#sns.barplot(x_pos, word_counts, palette='hls')\n",
    "#plt.xticks(x_pos, words, rotation=90) \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the whole collection to a single string to use in the wordcloud\n",
    "# can make this later for each cluster if we want\n",
    "docs = pd.DataFrame(processed_abstracts)\n",
    "allabstracts = \"\"\n",
    "for i in processed_abstracts:\n",
    "    for j in i:\n",
    "        allabstracts += j\n",
    "        allabstracts += ' '\n",
    "\n",
    "    \n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=200)\n",
    "wordcloud.generate(allabstracts)\n",
    "wordcloud.to_image()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.figure(figsize=[20,20])\n",
    "#plt.imshow(wordcloud, interpolation=\"sinc\")\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "# First fit the dict abstract_bow using the tfidfmodel \n",
    "tfidf_model = TfidfModel(abstract_bow)\n",
    "\n",
    "# now apply the tfidf_model to the documents\n",
    "corpus_tfidf = tfidf_model[abstract_bow]   # can here just try to apply to the first doc in corpus to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(abstract_bow, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0      Word:  0.004*\"patient\" + 0.004*\"qingdao\" + 0.004*\"perpetu\" + 0.004*\"grant\" + 0.004*\"right\" + 0.004*\"reus\" + 0.004*\"permiss\" + 0.004*\"reserv\" + 0.003*\"medrxiv\" + 0.003*\"display\"\n",
      "\n",
      "Topic: 1      Word:  0.004*\"r\" + 0.004*\"protein\" + 0.004*\"nsp\" + 0.003*\"vaccin\" + 0.003*\"secondari\" + 0.002*\"predict\" + 0.002*\"number\" + 0.002*\"heterogen\" + 0.002*\"moment\" + 0.002*\"emerg\"\n",
      "\n",
      "Topic: 2      Word:  0.005*\"contact\" + 0.003*\"social\" + 0.003*\"distanc\" + 0.003*\"contacttrac\" + 0.002*\"transmiss\" + 0.002*\"intervent\" + 0.002*\"control\" + 0.002*\"epidem\" + 0.002*\"covid\" + 0.002*\"suscept\"\n",
      "\n",
      "Topic: 3      Word:  0.004*\"astrovirus\" + 0.004*\"filter\" + 0.004*\"mutat\" + 0.003*\"nvis\" + 0.003*\"bind\" + 0.003*\"recoveri\" + 0.002*\"compound\" + 0.002*\"niv\" + 0.002*\"druglik\" + 0.002*\"priorit\"\n",
      "\n",
      "Topic: 4      Word:  0.001*\"wuhan\" + 0.001*\"ncov\" + 0.001*\"protein\" + 0.001*\"outbreak\" + 0.001*\"preprint\" + 0.001*\"new\" + 0.001*\"covid\" + 0.001*\"high\" + 0.001*\"perpetu\" + 0.001*\"identifi\"\n",
      "\n",
      "Topic: 5      Word:  0.006*\"frameshift\" + 0.004*\"patient\" + 0.003*\"travel\" + 0.003*\"henan\" + 0.002*\"covid\" + 0.002*\"restrict\" + 0.002*\"translat\" + 0.002*\"incub\" + 0.002*\"rna\" + 0.002*\"mobil\"\n",
      "\n",
      "Topic: 6      Word:  0.004*\"correct\" + 0.003*\"refuge\" + 0.003*\"mngs\" + 0.003*\"popul\" + 0.003*\"vaccin\" + 0.003*\"bed\" + 0.003*\"technolog\" + 0.003*\"ncov\" + 0.002*\"measl\" + 0.002*\"design\"\n",
      "\n",
      "Topic: 7      Word:  0.004*\"sulfat\" + 0.003*\"heparin\" + 0.003*\"heparan\" + 0.003*\"polysaccharid\" + 0.002*\"surfac\" + 0.002*\"receptor\" + 0.002*\"interact\" + 0.001*\"bind\" + 0.001*\"domain\" + 0.001*\"hsr\"\n",
      "\n",
      "Topic: 8      Word:  0.004*\"pro\" + 0.004*\"]\" + 0.004*\"[\" + 0.003*\"admit\" + 0.003*\"patient\" + 0.003*\"januari\" + 0.003*\"drug\" + 0.002*\"nelfinavir\" + 0.002*\"kcalmol\" + 0.002*\"dock\"\n",
      "\n",
      "Topic: 9      Word:  0.004*\"ibv\" + 0.004*\"protein\" + 0.003*\"isol\" + 0.003*\"adapt\" + 0.003*\"virusinteract\" + 0.003*\"strategi\" + 0.002*\"mammal\" + 0.002*\"death\" + 0.002*\"focus\" + 0.002*\"sampl\"\n",
      "\n",
      "Topic: 10      Word:  0.004*\"end\" + 0.004*\"citi\" + 0.004*\"background\" + 0.003*\"humantohuman\" + 0.003*\"ncov\" + 0.003*\"wuhan\" + 0.002*\"emerg\" + 0.002*\"caus\" + 0.002*\"china\" + 0.002*\"novel\"\n",
      "\n",
      "Topic: 11      Word:  0.004*\"cheer\" + 0.003*\"logist\" + 0.003*\"cpsrnas\" + 0.003*\"taxonom\" + 0.003*\"rnas\" + 0.003*\"psrnas\" + 0.003*\"sarscovcpsr\" + 0.003*\"palindrom\" + 0.002*\"size\" + 0.002*\"south\"\n",
      "\n",
      "Topic: 12      Word:  0.002*\"sadscov\" + 0.002*\"coronavirus\" + 0.002*\"twirl\" + 0.002*\"januari\" + 0.002*\"wuhan\" + 0.002*\"hku\" + 0.002*\"porcin\" + 0.002*\"inform\" + 0.002*\"bat\" + 0.002*\"confirm\"\n",
      "\n",
      "Topic: 13      Word:  0.004*\"cost\" + 0.004*\"sirna\" + 0.003*\"cl\" + 0.003*\"deliveri\" + 0.003*\"plant\" + 0.003*\"proteas\" + 0.003*\"peak\" + 0.002*\"total\" + 0.002*\"approach\" + 0.002*\"measur\"\n",
      "\n",
      "Topic: 14      Word:  0.004*\"morbid\" + 0.004*\"zika\" + 0.004*\"artd\" + 0.003*\"lifespan\" + 0.003*\"america\" + 0.003*\"cohort\" + 0.003*\"rok\" + 0.003*\"year\" + 0.002*\"meet\" + 0.002*\"rap\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows 10 topics extracted from the abstracts\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {}      Word:  {}'.format(idx, topic) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
