## Nerual netowrk

* Feed input via nodes
* Logistic unit does computation
* Sends output

### Activation functions
#### Sigmoid
* Tipically used for final layer (Intermediate layers can have other activation functions)
* $g(z) = \frac{1}{1+e^{-z}}$
* $g'(z) = g(z) (1-g(z))$
#### Hypterbolic Tangent (Tanh)
* Scaled version of sigmoid (Symmtric over the origin) [-1 to 1]
* $g(z) = \frac{e^z-e^-z}{e^z+e^{-z}}$
* $g'(z) = 1-g(z)^2$
#### Rectified Linear Unit RELU
* It does not activate all the neurons at the same time
* Sigmoid and tanh can make  descent slow if value positively large or negatively small
* $g(z) = max(0,z)
* $\left\{\begin{matrix}  1 & z>0 \\   0 & otherwise  \end{matrix}\right.$
#### Leaky ReLU
* When ReLU has gradients moving towards zero
* Can create dead neurons which never get activated
* $g(z)=max(0.01z,x)$
* $\left\{\begin{matrix}  0.01 & if \space  z < 0 \\   1 & if \space z\geq 1  \end{matrix}\right.$
#### Softmax function
* Similar to sigmoid but handy when handleing classification problems
* $\sigma(z)_j=\frac{e^{z_j}}{\sum_1^k e^{z_k}}$

### Caveats
* Sigmoid work better in classifiers
* Sigmoid and tanh area avoided due to vanishing gradient problem
* ReLU function is a general activation function and is used mostly
* Use leaky ReLU if dead neurons found
* Only use ReLU in hidden layers
* Start using ReLU and move to other activation functions if ReLU does not provide optimum results




### Perceptron
Most basic representation of a neural network

### Cost function
Establishes how wrong the predicted value is from its original by measuring the distance between these two values.
### Graient descent
Is used to predict "how much" should weights and biases be adjusted in order to predict more accurately the new values for the NN to be trained in the next iteration.
### Learning rates
Avoid convergence in local minima for small steps, adjust steps nad rate during iterations.
Fix learning rates based on size of gradient, rate of learning and size of particular weights

* Momentum
* Adgrad
* Adadelta
* Adam
* RMSProp

#### Mini-batches
* Computed the gradients for a batch of points rather than each one
* Smoother convergence, allows for larger learning rates
* Can paralelize computation , fulle use GPU POTENTIAL
![Mini Batch demonstration image](img/mini-batch.png)


#### Algorithm
1. Initialize weights randomly $\sim N(o,\sigma^2)$
2. Loop until convergence
3. Compute graient $\frac{\partial J(\theta)}{\partial\theta}$ 
4. Update weights $\theta=\theta-\alpha\frac{\partial J(\theta)}{\partial\theta}$
5. Return new weights

* Forward propagation
* Adjust weights during backward propagation


### Summary
* Deep NN are compositions of perceptrons
* Actvtions povide non-linearity
* Back propagation for parameter learning
* Pay attention to learning rates
* Use mini batches
* Regularization using dropouts and early stopping

## Optimization

#### Overfitting and regularization
* Deep NN gave a large capacity and tend to overfit (complex models)
* Regularization forces the model to learn a simpler model
##### Regularization strategies
###### Dropout
*  Drop neurons by setting activation  = 0
* Forces network to learn using a smaller capacity -> Robust to overfitting
###### Dropout Early stoppoing (optional)
* Stop training before there is a chance to overfit

#### Gradient Descent with momentum
* Update W with exponentially weighted running average of dW
* Faster than standarde gradient descent
* By averaging the values, it smoothens the oscilation in the gradient descent
* dW = Exponentialy weighted moving average. Itizialized to 0 and updated on every iteration
* $V_{dW}=\beta V_{dW}+(1-\beta)dW \space W = W-\alpha V{dW}$

#### RMSprop (Root mean square prop)
* Dampen large oscillation in some particular drections by penalizing movements that are large (dividing it by square root of itself)
#### Adam (Adaptive moment estimation)
* Combines omentum and RMS Prop ptimizations
* Mommentum part with bias correction

#### Learning rate decay
* Can be used to speed up learning
* $\alpha=\frac{1}{1+decayRate \space * \space epochNum}\alpha_0$

#### Batch normalization
* Normalize the value of Z in any hidden layer
* Makes values not fluctuate much between iterations
* Makes subsequent layers better

#### Bias and variance
* USE REgularization
* How to identify if train and test set have different distribution for bias or variance?
* Large $\lambda$ = High bias (underfit)
* Intermediate $\lambda$ = Just right
* Small $\lambda$ = High variance(overfit)


##### Gradient checking
* Double check gradient calculations are correct
* Use sparingly as it is very slow
* For each feature calculate approximate gradeint
* Calculate difference between actual gradient calculated suring gradient descent.

##### Learning curves
* Useful to plot for algorihmic sanity check


